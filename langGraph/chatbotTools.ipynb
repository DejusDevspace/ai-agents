{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tools\n",
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97b2a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arxiv\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=2, doc_content_chars_max=500)\n",
    "arxiv = ArxivQueryRun(api_wrapper=api_wrapper_arxiv, description=\"Query arxiv papers\")\n",
    "print(arxiv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620192fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-07-22\\nTitle: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\\nAuthors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\\nSummary: The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time o\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv.invoke(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=500)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper_wiki, description=\"Search wikipedia for information\")\n",
    "print(wiki.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b9f6fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c137e496",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tavily search tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3152b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tavily = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8a4ac2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'MCP Explained: The New Standard Connecting AI to Everything',\n",
       "  'url': 'https://medium.com/@elisowski/mcp-explained-the-new-standard-connecting-ai-to-everything-79c5a1c98288',\n",
       "  'content': 'Model Context Protocol (MCP) is an open standard developed by Anthropic, the company behind Claude. While it may sound technical, but the core idea is simple: give AI agents a consistent way to connect with tools, services, and data — no matter where they live or how they’re built. [...] As highlighted in Forbes, MCP is a big leap forward in how AI agents operate. Instead of just answering questions, agents can now perform useful, multi-step tasks — like retrieving data, summarizing documents, or saving content to a file.\\n\\nBefore MCP, each of those actions required a unique API, custom logic, and developer time to glue it all together. [...] AI agents can write code, summarize reports, even chat like humans — but when it’s time to actually do something in the real world, they stall.\\n\\nWhy? Because most tools still need clunky, one-off integrations.\\n\\nMCP (Model Context Protocol) changes that. It gives AI agents a simple, standardized way to plug into tools, data, and services — no hacks, no hand-coding.\\n\\nWith MCP, AI goes from smart… to actually useful.\\n\\n# What Is MCP, Really?',\n",
       "  'score': 0.7746691},\n",
       " {'title': 'What is MCP (Model Context Protocol)? - Zapier',\n",
       "  'url': 'https://zapier.com/blog/mcp/',\n",
       "  'content': \"MCP is a standard framework that defines how AI systems can interact with external tools, services, and data sources. Instead of having to create custom integrations for every service, MCP defines the basics of how they should interoperate, how requests are structured, what features are available, and how they can be discovered. It enables developers to easily and reliably build secure, two-way connections between AI tools and external data sources, apps, and other services. [...] It's an open source protocol designed to safely and securely connect AI tools to data sources like your company's CRM, Slack workspace, or dev server. That means your AI assistant can pull in relevant data and trigger actions in those tools—like updating a record, sending a message, or kicking off a deployment. By giving AI assistants the power to both understand and act, MCP enables more useful, context-aware, and proactive AI experiences. [...] If you're a developer building an AI app and want it to be able to initiate tasks in other apps—not just access data—then MCP is well worth a look. It gives your AI tools the ability to trigger actions in external systems, like sending messages, creating records, or kicking off workflows, all through a standardized protocol. And if you don't want to create your own server, you can find a list of available MCP servers here.\",\n",
       "  'score': 0.75659186},\n",
       " {'title': 'MCP: What It Is and Why It Matters - Elevate | Addy Osmani - Substack',\n",
       "  'url': 'https://addyo.substack.com/p/mcp-what-it-is-and-why-it-matters',\n",
       "  'content': \"In a nutshell, MCP is like giving your AI assistant a universal remote control to operate all your digital devices and services. Instead of being stuck in its own world, your AI can now reach out and press the buttons of other applications safely and intelligently. This common protocol means one AI can integrate with thousands of tools as long as those tools have an MCP interface – eliminating the need for custom integrations for each new app. The result: your AI helper becomes far more [...] Imagine you have a single universal plug that fits all your devices – that's essentially what the Model Context Protocol (MCP) is for AI. MCP is an open standard (think “USB-C for AI integrations”) that allows AI models to connect to many different apps and data sources in a consistent way. In simple terms, MCP lets an AI assistant talk to various software tools using a common language, instead of each tool requiring a different adapter or custom code.\\n\\nImage 9 [...] So, what does this mean in practice? If you’re using an AI coding assistant like Cursor or Windsurf, MCP is the shared protocol that lets that assistant use external tools on your behalf. For example, with MCP an AI model could fetch information from a database, edit a design in Figma, or control a music app – all by sending natural-language instructions through a standardized interface. You (or the AI) no longer need to manually switch contexts or learn each tool’s API; the MCP “translator”\",\n",
       "  'score': 0.740494},\n",
       " {'title': 'What Is MCP, and Why Is Everyone – Suddenly!– Talking About It?',\n",
       "  'url': 'https://huggingface.co/blog/Kseniase/mcp',\n",
       "  'content': 'MCP is not itself an \"agent framework\"; rather, it acts as a standardized integration layer for agents. MCP is all about the Action part – specifically, giving agents a standardized way to perform actions involving external data or tools. It provides the plumbing that connects an AI agent to the outside world in a secure, structured manner. Without MCP (or something like it), every time an agent needs to do something in the world – whether fetching a file, querying a database, or invoking an [...] To simplify that, Anthropic came up with Model Context Protocol (MCP) – an open standard designed to bridge AI assistants with the world of data and tools, to plug in many different sources of context. They announced it in November 2024. The reaction was sort of blah. But now MCP is trending, already passing Langchain and promising to overcome OpenAPI and CrewAI pretty soon. Major AI players and open-source communities are rallying around MCP, seeing it as a potential game-changer for building [...] It’s important to highlight again that MCP is not an orchestration engine or agent brain by itself. Rather, it’s an integration layer within an agentic architecture. It complements agent orchestration tools like LangChain, LangGraph, CrewAI, or LlamaIndex by serving as a unified \"toolbox\" from which AI agents can invoke external actions. Instead of replacing orchestration – which determines when and why an agent uses a tool – MCP defines how these tools are called and information exchanged.',\n",
       "  'score': 0.7187726},\n",
       " {'title': 'Definition of Microsoft Certified Professional (MCP) - Gartner',\n",
       "  'url': 'https://www.gartner.com/en/information-technology/glossary/mcp-microsoft-certified-professional',\n",
       "  'content': 'Microsoft Certified Professional (MCP) is a Microsoft certification that requires successful completion of qualifying exams.',\n",
       "  'score': 0.7099254}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily.invoke(\"Tell me about MCP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46a99813",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combining tools\n",
    "tools = [arxiv, wiki, tavily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1ae59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fb57a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the LLM model\n",
    "llm = ChatGroq(model=\"qwen-qwq-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ec72f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, so I need to explain what a brace is in football. Let me start by recalling what I know. I think a brace refers to a player scoring two goals in a single match. But I should make sure that\\'s accurate. I remember hearing commentators say things like \"the player got a brace\" after they score two goals. Maybe it\\'s from the idea of \"two of something,\" like putting two items in a brace. \\n\\nWait, but where does the term actually come from? I\\'m not entirely sure about the origin. Maybe it\\'s from British English? The term \"brace\" in general English means a pair of something, like a brace of guns or pheasants. So applying that to two goals makes sense. \\n\\nBut I should also check if there are any exceptions or different uses. For example, does it only apply to goals, or could it be used for other actions like assists? Probably just goals, since I\\'ve only heard it in the context of scoring. Also, is there a difference between a brace and a hat-trick? A hat-trick is three goals, so brace is two. \\n\\nI should also consider examples. Like when a player scores two goals in a match, they\\'re said to have scored a brace. Maybe mention some famous players who have done this? Not sure if necessary, but maybe a quick example would help clarify.\\n\\nWait, are there different types of braces? Like, maybe a \"perfect brace\" if they\\'re two different types of goals, but I don\\'t think that\\'s standard. Maybe that\\'s just a colloquial way to describe it. The main point is two goals. \\n\\nAlso, in some contexts, like in other sports, does \"brace\" mean something else? Probably not. It\\'s mostly a football (soccer) term. \\n\\nI should structure this explanation: define the term, explain the origin, give an example, and maybe contrast with hat-trick. Maybe mention that it\\'s common in British football terminology. \\n\\nHmm, need to make sure there\\'s no confusion with other terms. For instance, \"double\" sometimes refers to a team winning two trophies, but not a player\\'s goals. So \"brace\" is specifically about two goals by a single player. \\n\\nAnother point: sometimes a brace can be consecutive goals, but I think it\\'s just two in total, regardless of when they were scored. \\n\\nAlright, I think that covers the basics. Let me put it all together in a clear, concise way without any errors.\\n</think>\\n\\nA **brace** in football refers to a player scoring **two goals** in a single match. The term originates from the general English use of \"brace,\" which means a pair or set of two (e.g., a brace of ducks or guns). In the context of football, it highlights a player’s achievement of netting twice during a game.\\n\\n### Key Points:\\n1. **Definition**: Scoring two goals in one match.  \\n2. **Origin**: The word derives from the archaic British term for a pair of items.  \\n3. **Usage**: Common in British football commentary and media.  \\n4. **Contrast with \"Hat-Trick\"**: While a brace denotes two goals, a \"hat-trick\" refers to three goals in a single match.  \\n5. **Example**: If a striker scores a penalty in the first half and a headers in the second, they’ve earned a brace.  \\n\\n### Example in Context:\\n- \"Harry Kane netted a brace against Chelsea, securing a crucial victory for Tottenham.\"  \\n\\nThe term is purely celebratory and emphasizes the player’s contribution, though it has no official bearing on league or cup standings. It’s a popular way to acknowledge a strong individual performance.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 766, 'prompt_tokens': 17, 'total_tokens': 783, 'completion_time': 1.882466969, 'prompt_time': 0.003298996, 'queue_time': 0.160373177, 'total_time': 1.885765965}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_1e88ca32eb', 'finish_reason': 'stop', 'logprobs': None}, id='run--cb8e7576-eae4-4978-b100-d712b104a640-0', usage_metadata={'input_tokens': 17, 'output_tokens': 766, 'total_tokens': 783})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is a brace in football?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e66df27",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8b03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_with_tools.invoke(\"What is the recent news on AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb492c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking for recent news on AI. Let me see which tool I can use here. The available functions are arxiv, wikipedia, and tavily_search_results_json. \\n\\nArxiv is more for academic papers, so maybe not the best for recent news. Wikipedia is for encyclopedic entries, which also might not have the latest updates. Tavily\\'s search results, though, are meant for current information. That\\'s probably the right choice here.\\n\\nI should structure the tool call with Tavily\\'s function and the query as \"recent news on AI\". The parameters need to be a JSON object with the query. Let me make sure the syntax is correct. The name should be \"tavily_search_results_json\" and the arguments should have \"query\": \"recent news on AI\". \\n\\nDouble-checking that the function name matches exactly as provided. Yep, that\\'s right. So the final tool call should be in the specified JSON format within the tool_call tags.\\n', 'tool_calls': [{'id': '4mpernjca', 'function': {'arguments': '{\"query\":\"recent news on AI\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 233, 'prompt_tokens': 269, 'total_tokens': 502, 'completion_time': 0.574194966, 'prompt_time': 0.015939493, 'queue_time': 0.16178105, 'total_time': 0.590134459}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_98b01f25b2', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--ad2f0bcf-cac9-442d-9aab-89364d2d5bd9-0' tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'recent news on AI'}, 'id': '4mpernjca', 'type': 'tool_call'}] usage_metadata={'input_tokens': 269, 'output_tokens': 233, 'total_tokens': 502}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a04f5192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking about the latest research on quantum computing. I need to find the most recent studies or papers on this topic. The tools available are arxiv, wikipedia, and Tavily search. \\n\\nFirst, arXiv is a preprint server where researchers often upload their latest papers before they\\'re published in journals. That\\'s probably the best source for the most recent work. Wikipedia might have general info but might not be up-to-date with the latest research. Tavily\\'s search results could provide some summaries or articles, but arXiv direct access would be more authoritative here.\\n\\nSo I should use the arXiv function with a query like \"quantum computing\" to get the latest preprints. Let me check the parameters: the function requires a query string. I\\'ll structure the tool call accordingly.\\n', 'tool_calls': [{'id': 'jnt98c4m2', 'function': {'arguments': '{\"query\":\"quantum computing\"}', 'name': 'arxiv'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 271, 'total_tokens': 463, 'completion_time': 0.481783275, 'prompt_time': 0.016166814, 'queue_time': 0.160420488, 'total_time': 0.497950089}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_98b01f25b2', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--ca699be4-276e-405c-82bd-4648f16415e9-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'quantum computing'}, 'id': 'jnt98c4m2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 192, 'total_tokens': 463})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is the latest research on quantum computing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8879a854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is machine learning?\" I need to figure out the best way to answer this. Let me see which tools are available. There\\'s arxiv, wikipedia, and Tavily search.\\n\\nSince the user is asking for a definition or explanation of machine learning, Wikipedia would be a good start because it usually provides clear, concise explanations. The Wikipedia function requires a query parameter, so I should use \"machine learning\" as the query. That should fetch the Wikipedia page on machine learning, which likely has a detailed yet accessible introduction. \\n\\nAlternatively, Tavily\\'s search results might give a broader range of sources, but Wikipedia is more reliable for a foundational explanation. The arXiv function is more for research papers, which might be too technical for a basic question. \\n\\nI\\'ll go with Wikipedia first. Let me structure the tool call properly. The function name is \"wikipedia\" and the argument is {\"query\": \"machine learning\"}. I need to make sure the JSON is correctly formatted. Alright, that should do it.\\n', 'tool_calls': [{'id': 'gzp5d4sta', 'function': {'arguments': '{\"query\":\"machine learning\"}', 'name': 'wikipedia'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 238, 'prompt_tokens': 266, 'total_tokens': 504, 'completion_time': 0.547694646, 'prompt_time': 0.019603554, 'queue_time': 0.18919354, 'total_time': 0.5672982}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_28178d7ff6', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--bca39616-9bf1-48f3-8950-be9dc162b000-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'machine learning'}, 'id': 'gzp5d4sta', 'type': 'tool_call'}], usage_metadata={'input_tokens': 266, 'output_tokens': 238, 'total_tokens': 504})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ef221",
   "metadata": {},
   "source": [
    "## LANGGRAPH WORKFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages  ## add_messages is a reducer (appends messages instead of overriding variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94032119",
   "metadata": {},
   "outputs": [],
   "source": [
    "## State schema\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db13d421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20d39dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Ghost\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Ghost! Nice to meet you. How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific! 😊\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": HumanMessage(content=\"Hello, my name is Ghost\")})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae2a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
